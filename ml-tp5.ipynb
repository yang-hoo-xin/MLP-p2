{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":631948,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":476322,"modelId":492243}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Baseline with Logistic Regression","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T13:33:25.334231Z","iopub.execute_input":"2025-11-06T13:33:25.335347Z","iopub.status.idle":"2025-11-06T13:33:25.343649Z","shell.execute_reply.started":"2025-11-06T13:33:25.335307Z","shell.execute_reply":"2025-11-06T13:33:25.342375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. 导入必要的库\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import log_loss, classification_report, confusion_matrix\nimport string\n\n# 2. 加载数据\ntrain = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\n# 3. 特征工程：提取统计特征\n\ndef create_features(df):\n    # 长度特征\n    df['prompt_length'] = df['prompt'].str.len()\n    df['response_a_length'] = df['response_a'].str.len()\n    df['response_b_length'] = df['response_b'].str.len()\n    # 单词数\n    df['prompt_word_count'] = df['prompt'].str.split().str.len()\n    df['response_a_word_count'] = df['response_a'].str.split().str.len()\n    df['response_b_word_count'] = df['response_b'].str.split().str.len()\n    # 标点数\n    df['prompt_punc_count'] = df['prompt'].str.count(f'[{string.punctuation}]')\n    df['response_a_punc_count'] = df['response_a'].str.count(f'[{string.punctuation}]')\n    df['response_b_punc_count'] = df['response_b'].str.count(f'[{string.punctuation}]')\n    # 差异特征\n    df['response_length_diff'] = df['response_a_length'] - df['response_b_length']\n    df['response_word_diff'] = df['response_a_word_count'] - df['response_b_word_count']\n    return df\n\ntrain = create_features(train)\ntest = create_features(test)\n\n# 4. Label编码（可选：对model名称，但test不会用到）\nle = LabelEncoder()\nmodel_cols = []\nfor col in ['model_a', 'model_b']:\n    train[f'{col}_enc'] = le.fit_transform(train[col])\n    model_cols += [f'{col}_enc']\n\n# 5. 构造标签\n#   winner_model_a=1→类别0，winner_model_b=1→类别1，winner_tie=1→类别2\ntrain['target'] = train[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)\n\n# 6. 选定全部数值特征\nfeature_cols = [\n    'prompt_length', 'response_a_length', 'response_b_length',\n    'prompt_word_count', 'response_a_word_count', 'response_b_word_count',\n    'prompt_punc_count', 'response_a_punc_count', 'response_b_punc_count',\n    'response_length_diff', 'response_word_diff'\n    # 若希望，也可添加model_a_enc/model_b_enc\n]\n\nX = train[feature_cols]\ny = train['target']\n\n# 7. 数据拆分 + 标准化\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\n\n# 8. 逻辑回归训练\nclf = LogisticRegression(max_iter=1000, random_state=42, multi_class='ovr')\nclf.fit(X_train_scaled, y_train)\n\n# 9. 验证集效果\nval_pred_proba = clf.predict_proba(X_val_scaled)\nval_pred = clf.predict(X_val_scaled)\nprint('Validation Log Loss:', log_loss(y_val, val_pred_proba))\nprint('Classification Report:\\n', classification_report(y_val, val_pred, digits=4))\n\n# 10. 生成test特征、标准化并预测\nX_test = test[feature_cols]\nX_test_scaled = scaler.transform(X_test)\ntest_pred_proba = clf.predict_proba(X_test_scaled)\n\n# 11. 生成Kaggle提交文件\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': test_pred_proba[:,0],\n    'winner_model_b': test_pred_proba[:,1],\n    'winner_tie': test_pred_proba[:,2],\n})\nsubmission.to_csv('submission_baseline.csv', index=False)\nprint(submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T13:33:25.346133Z","iopub.execute_input":"2025-11-06T13:33:25.346402Z","iopub.status.idle":"2025-11-06T13:33:37.557668Z","shell.execute_reply.started":"2025-11-06T13:33:25.346384Z","shell.execute_reply":"2025-11-06T13:33:37.555398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 2: Embedding-based model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T13:33:37.558373Z","iopub.execute_input":"2025-11-06T13:33:37.558657Z","iopub.status.idle":"2025-11-06T13:33:37.564414Z","shell.execute_reply.started":"2025-11-06T13:33:37.558633Z","shell.execute_reply":"2025-11-06T13:33:37.563056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\ntrain = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\ndef concat_text(df):\n    return (\n        df['prompt'].astype(str) + ' ' + df['response_a'].astype(str),\n        df['prompt'].astype(str) + ' ' + df['response_b'].astype(str)\n    )\ntrain_a, train_b = concat_text(train)\ntest_a, test_b = concat_text(test)\n\nemb_a = model.encode(train_a.tolist(), batch_size=32, show_progress_bar=True)\nemb_b = model.encode(train_b.tolist(), batch_size=32, show_progress_bar=True)\nX = np.hstack([emb_a, emb_b, emb_a - emb_b])\ny = train[['winner_model_a','winner_model_b','winner_tie']].values.argmax(axis=1)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nclf = LogisticRegression(max_iter=200)\nclf.fit(X_train, y_train)\nprint('Val Log Loss:', log_loss(y_val, clf.predict_proba(X_val)))\nemb_a_test = model.encode(test_a.tolist(), batch_size=32, show_progress_bar=True)\nemb_b_test = model.encode(test_b.tolist(), batch_size=32, show_progress_bar=True)\nX_test = np.hstack([emb_a_test, emb_b_test, emb_a_test - emb_b_test])\nproba = clf.predict_proba(X_test)\n\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': proba[:,0],\n    'winner_model_b': proba[:,1],\n    'winner_tie': proba[:,2]\n})\nsubmission.to_csv('submission_emb.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T13:52:32.569438Z","iopub.execute_input":"2025-11-06T13:52:32.569713Z","iopub.status.idle":"2025-11-06T13:54:40.294350Z","shell.execute_reply.started":"2025-11-06T13:52:32.569691Z","shell.execute_reply":"2025-11-06T13:54:40.293154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Final Model (TF-IDF + Embedding + Calibration + Ensemble)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T13:35:58.836602Z","iopub.status.idle":"2025-11-06T13:35:58.836961Z","shell.execute_reply.started":"2025-11-06T13:35:58.836783Z","shell.execute_reply":"2025-11-06T13:35:58.836794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Step 5: Final Model (Baseline + Embeddings) with Weight Search Ensemble ===\nimport os, string, numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\n\n# 1) 读取数据（和前面一致）\nTRAIN_PATH = '/kaggle/input/llm-classification-finetuning/train.csv'\nTEST_PATH  = '/kaggle/input/llm-classification-finetuning/test.csv'\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\n\n# 2) 标签：winner_model_a=0, winner_model_b=1, winner_tie=2\ny = train[['winner_model_a','winner_model_b','winner_tie']].values.argmax(axis=1)\n\n# 3) 统一一次划分，保证两种模型在同一验证集上对比\ntrain_idx, val_idx = train_test_split(\n    np.arange(len(train)),\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\ny_train, y_val = y[train_idx], y[val_idx]\n\n# -----------------------------\n# A) Baseline（数值统计特征 + LR）\n# -----------------------------\ndef create_features(df):\n    out = pd.DataFrame(index=df.index)\n    out['prompt_length']      = df['prompt'].astype(str).str.len()\n    out['response_a_length']  = df['response_a'].astype(str).str.len()\n    out['response_b_length']  = df['response_b'].astype(str).str.len()\n\n    out['prompt_word_count']     = df['prompt'].astype(str).str.split().str.len()\n    out['response_a_word_count'] = df['response_a'].astype(str).str.split().str.len()\n    out['response_b_word_count'] = df['response_b'].astype(str).str.split().str.len()\n\n    out['prompt_punc_count']     = df['prompt'].astype(str).str.count(f'[{string.punctuation}]')\n    out['response_a_punc_count'] = df['response_a'].astype(str).str.count(f'[{string.punctuation}]')\n    out['response_b_punc_count'] = df['response_b'].astype(str).str.count(f'[{string.punctuation}]')\n\n    out['response_length_diff'] = out['response_a_length'] - out['response_b_length']\n    out['response_word_diff']   = out['response_a_word_count'] - out['response_b_word_count']\n    return out\n\nX_all_base  = create_features(train)\nX_test_base = create_features(test)\n\nXtr_base = X_all_base.iloc[train_idx]\nXva_base = X_all_base.iloc[val_idx]\n\nscaler = StandardScaler()\nXtrb = scaler.fit_transform(Xtr_base)\nXvab = scaler.transform(Xva_base)\nXteb  = scaler.transform(X_test_base)\n\nclf_base = LogisticRegression(max_iter=1000, random_state=42, multi_class='ovr')\nclf_base.fit(Xtrb, y_train)\n\nproba_val_base = clf_base.predict_proba(Xvab)\nproba_test_base = clf_base.predict_proba(Xteb)\nll_base = log_loss(y_val, proba_val_base)\nprint(f\"[Baseline] Val logloss: {ll_base:.6f}\")\n\n# -----------------------------\n# B) Embedding（MiniLM + LR）\n# -----------------------------\nuse_embedding = True\nproba_val_emb = None\nproba_test_emb = None\n\nif use_embedding:\n    try:\n        from sentence_transformers import SentenceTransformer\n\n        def concat_text(df):\n            # 简单拼接 prompt + response_a / response_b\n            a = (df['prompt'].astype(str) + ' ' + df['response_a'].astype(str)).tolist()\n            b = (df['prompt'].astype(str) + ' ' + df['response_b'].astype(str)).tolist()\n            return a, b\n\n        model_name = 'sentence-transformers/all-MiniLM-L12-v2'  # 与你 step2 一致\n        st_model = SentenceTransformer(model_name)\n\n        # 只对训练划分/验证划分分别编码，保证与统一划分一致\n        a_all, b_all = concat_text(train)\n        a_test, b_test = concat_text(test)\n\n        a_tr = [a_all[i] for i in train_idx]\n        b_tr = [b_all[i] for i in train_idx]\n        a_va = [a_all[i] for i in val_idx]\n        b_va = [b_all[i] for i in val_idx]\n\n        emb_a_tr = st_model.encode(a_tr, batch_size=32, show_progress_bar=True)\n        emb_b_tr = st_model.encode(b_tr, batch_size=32, show_progress_bar=True)\n        Xtr_emb  = np.hstack([emb_a_tr, emb_b_tr, emb_a_tr - emb_b_tr])\n\n        emb_a_va = st_model.encode(a_va, batch_size=32, show_progress_bar=True)\n        emb_b_va = st_model.encode(b_va, batch_size=32, show_progress_bar=True)\n        Xva_emb  = np.hstack([emb_a_va, emb_b_va, emb_a_va - emb_b_va])\n\n        clf_emb = LogisticRegression(max_iter=200, random_state=42)\n        clf_emb.fit(Xtr_emb, y_train)\n        proba_val_emb = clf_emb.predict_proba(Xva_emb)\n        ll_emb = log_loss(y_val, proba_val_emb)\n        print(f\"[Embedding] Val logloss: {ll_emb:.6f}\")\n\n        # test\n        emb_a_te = st_model.encode(a_test, batch_size=32, show_progress_bar=True)\n        emb_b_te = st_model.encode(b_test, batch_size=32, show_progress_bar=True)\n        Xte_emb  = np.hstack([emb_a_te, emb_b_te, emb_a_te - emb_b_te])\n        proba_test_emb = clf_emb.predict_proba(Xte_emb)\n\n    except Exception as e:\n        print(\"[Embedding] 加载/推理失败，将仅使用 Baseline。错误：\", repr(e))\n        use_embedding = False\n\n# -----------------------------\n# C) 权重融合（在同一验证集上网格搜索 alpha）\n# -----------------------------\nif use_embedding and (proba_val_emb is not None):\n    best_alpha, best_ll = 0.5, 1e9\n    for alpha in np.linspace(0, 1, 21):  # 0.00, 0.05, ..., 1.00\n        blend_val = alpha*proba_val_base + (1-alpha)*proba_val_emb\n        ll = log_loss(y_val, blend_val)\n        if ll < best_ll:\n            best_ll = ll\n            best_alpha = alpha\n    print(f\"[Ensemble] Best alpha={best_alpha:.2f}, Val logloss={best_ll:.6f}\")\n\n    # 最终融合 test\n    proba_test_final = best_alpha*proba_test_base + (1-best_alpha)*proba_test_emb\nelse:\n    print(\"[Ensemble] 仅使用 Baseline 结果。\")\n    proba_test_final = proba_test_base\n\n# -----------------------------\n# D) 生成最终提交\n# -----------------------------\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': proba_test_final[:, 0],\n    'winner_model_b': proba_test_final[:, 1],\n    'winner_tie':     proba_test_final[:, 2],\n})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Saved -> submission.csv\")\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T13:57:48.125232Z","iopub.execute_input":"2025-11-06T13:57:48.125768Z","iopub.status.idle":"2025-11-06T13:59:33.035453Z","shell.execute_reply.started":"2025-11-06T13:57:48.125745Z","shell.execute_reply":"2025-11-06T13:59:33.034885Z"}},"outputs":[],"execution_count":null}]}
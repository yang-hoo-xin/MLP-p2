{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":631737,"sourceType":"modelInstanceVersion","modelInstanceId":476142,"modelId":492061}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Baseline with Logistic Regression","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. 导入必要的库\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import log_loss, classification_report, confusion_matrix\nimport string\n\n# 2. 加载数据\ntrain = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\n# 3. 特征工程：提取统计特征\n\ndef create_features(df):\n    # 长度特征\n    df['prompt_length'] = df['prompt'].str.len()\n    df['response_a_length'] = df['response_a'].str.len()\n    df['response_b_length'] = df['response_b'].str.len()\n    # 单词数\n    df['prompt_word_count'] = df['prompt'].str.split().str.len()\n    df['response_a_word_count'] = df['response_a'].str.split().str.len()\n    df['response_b_word_count'] = df['response_b'].str.split().str.len()\n    # 标点数\n    df['prompt_punc_count'] = df['prompt'].str.count(f'[{string.punctuation}]')\n    df['response_a_punc_count'] = df['response_a'].str.count(f'[{string.punctuation}]')\n    df['response_b_punc_count'] = df['response_b'].str.count(f'[{string.punctuation}]')\n    # 差异特征\n    df['response_length_diff'] = df['response_a_length'] - df['response_b_length']\n    df['response_word_diff'] = df['response_a_word_count'] - df['response_b_word_count']\n    return df\n\ntrain = create_features(train)\ntest = create_features(test)\n\n# 4. Label编码（可选：对model名称，但test不会用到）\nle = LabelEncoder()\nmodel_cols = []\nfor col in ['model_a', 'model_b']:\n    train[f'{col}_enc'] = le.fit_transform(train[col])\n    model_cols += [f'{col}_enc']\n\n# 5. 构造标签\n#   winner_model_a=1→类别0，winner_model_b=1→类别1，winner_tie=1→类别2\ntrain['target'] = train[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)\n\n# 6. 选定全部数值特征\nfeature_cols = [\n    'prompt_length', 'response_a_length', 'response_b_length',\n    'prompt_word_count', 'response_a_word_count', 'response_b_word_count',\n    'prompt_punc_count', 'response_a_punc_count', 'response_b_punc_count',\n    'response_length_diff', 'response_word_diff'\n    # 若希望，也可添加model_a_enc/model_b_enc\n]\n\nX = train[feature_cols]\ny = train['target']\n\n# 7. 数据拆分 + 标准化\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\n\n# 8. 逻辑回归训练\nclf = LogisticRegression(max_iter=1000, random_state=42, multi_class='ovr')\nclf.fit(X_train_scaled, y_train)\n\n# 9. 验证集效果\nval_pred_proba = clf.predict_proba(X_val_scaled)\nval_pred = clf.predict(X_val_scaled)\nprint('Validation Log Loss:', log_loss(y_val, val_pred_proba))\nprint('Classification Report:\\n', classification_report(y_val, val_pred, digits=4))\n\n# 10. 生成test特征、标准化并预测\nX_test = test[feature_cols]\nX_test_scaled = scaler.transform(X_test)\ntest_pred_proba = clf.predict_proba(X_test_scaled)\n\n# 11. 生成Kaggle提交文件\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': test_pred_proba[:,0],\n    'winner_model_b': test_pred_proba[:,1],\n    'winner_tie': test_pred_proba[:,2],\n})\nsubmission.to_csv('submission_baseline.csv', index=False)\nprint(submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T09:41:02.203709Z","iopub.execute_input":"2025-11-06T09:41:02.204413Z","iopub.status.idle":"2025-11-06T09:41:14.304691Z","shell.execute_reply.started":"2025-11-06T09:41:02.204381Z","shell.execute_reply":"2025-11-06T09:41:14.303056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 2: Embedding-based model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)\n\ntrain = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\n# 用本地模型路径加载（路径需和你 Add Input 名称一致）\nmodel = SentenceTransformer('/kaggle/input/minilm-l12-v2-local/other/default/1/minilm_l12_v2_local')\n\n# 拼接文本\ndef concat_text(df):\n    return (\n        df['prompt'].astype(str) + ' ' + df['response_a'].astype(str),\n        df['prompt'].astype(str) + ' ' + df['response_b'].astype(str)\n    )\ntrain_a, train_b = concat_text(train)\ntest_a, test_b = concat_text(test)\n\nemb_a = model.encode(train_a.tolist(), batch_size=32, show_progress_bar=True)\nemb_b = model.encode(train_b.tolist(), batch_size=32, show_progress_bar=True)\nX = np.hstack([emb_a, emb_b, emb_a - emb_b])\ny = train[['winner_model_a','winner_model_b','winner_tie']].values.argmax(axis=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\nprint('Val Log Loss:', log_loss(y_val, clf.predict_proba(X_val)))\n\nemb_a_test = model.encode(test_a.tolist(), batch_size=32, show_progress_bar=True)\nemb_b_test = model.encode(test_b.tolist(), batch_size=32, show_progress_bar=True)\nX_test = np.hstack([emb_a_test, emb_b_test, emb_a_test - emb_b_test])\nproba = clf.predict_proba(X_test)\n\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': proba[:,0],\n    'winner_model_b': proba[:,1],\n    'winner_tie': proba[:,2]\n})\nsubmission.to_csv('submission_emb.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:05:09.086277Z","iopub.execute_input":"2025-11-06T11:05:09.087032Z","iopub.status.idle":"2025-11-06T11:10:15.798781Z","shell.execute_reply.started":"2025-11-06T11:05:09.087003Z","shell.execute_reply":"2025-11-06T11:10:15.798246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Step 3. Model Extensions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# Step 3: Model Extensions - Advanced Features and Ensemble Methods\n# =============================================================================\n\nprint(\"Starting Step 3: Model Extensions...\")\n\n# 安装必要的库（如果尚未安装）\n!pip install xgboost lightgbm --quiet\n\n# 导入所有必要的库\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import log_loss, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sentence_transformers import SentenceTransformer\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"所有库导入完成\")\n\n# 加载数据\nprint(\"加载数据...\")\ntrain = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\nprint(f\"训练集大小: {train.shape}\")\nprint(f\"测试集大小: {test.shape}\")\n\n# =============================================================================\n# 1. 偏差感知特征工程 (Bias-aware Features)\n# =============================================================================\n\ndef extract_bias_aware_features(df):\n    \"\"\"\n    提取位置偏差和冗长偏差相关特征\n    \"\"\"\n    print(\"提取偏差感知特征...\")\n    features = pd.DataFrame(index=df.index)\n    \n    # 位置偏差特征 (Position Bias)\n    features['position_a_first'] = 1  # response_a总是第一个\n    features['position_b_second'] = 0\n    \n    # 冗长偏差特征 (Verbosity Bias)\n    features['response_a_length'] = df['response_a'].str.len()\n    features['response_b_length'] = df['response_b'].str.len()\n    features['length_diff'] = features['response_a_length'] - features['response_b_length']\n    features['length_ratio'] = features['response_a_length'] / (features['response_b_length'] + 1)\n    \n    # 词汇丰富度特征\n    def lexical_richness(text):\n        if pd.isna(text) or text == '':\n            return 0\n        words = str(text).split()\n        if len(words) == 0:\n            return 0\n        return len(set(words)) / len(words)\n    \n    features['richness_a'] = df['response_a'].apply(lexical_richness)\n    features['richness_b'] = df['response_b'].apply(lexical_richness)\n    features['richness_diff'] = features['richness_a'] - features['richness_b']\n    \n    # 格式特征 (检查是否有列表、代码块等)\n    def format_complexity(text):\n        score = 0\n        text_str = str(text)\n        # 检查列表\n        if re.search(r'\\d+\\.|\\*|\\-', text_str):\n            score += 1\n        # 检查代码块\n        if '```' in text_str or '    ' in text_str:\n            score += 1\n        # 检查标题\n        if re.search(r'^#+\\s', text_str, re.MULTILINE):\n            score += 1\n        return score\n    \n    features['format_a'] = df['response_a'].apply(format_complexity)\n    features['format_b'] = df['response_b'].apply(format_complexity)\n    features['format_diff'] = features['format_a'] - features['format_b']\n    \n    # 问号和感叹号数量特征\n    features['question_a'] = df['response_a'].str.count(r'\\?')\n    features['question_b'] = df['response_b'].str.count(r'\\?')\n    features['exclamation_a'] = df['response_a'].str.count(r'!')\n    features['exclamation_b'] = df['response_b'].str.count(r'!')\n    \n    features['question_diff'] = features['question_a'] - features['question_b']\n    features['exclamation_diff'] = features['exclamation_a'] - features['exclamation_b']\n    \n    print(f\"偏差特征提取完成，特征数量: {features.shape[1]}\")\n    return features\n\n# 提取偏差特征\nbias_features_train = extract_bias_aware_features(train)\nbias_features_test = extract_bias_aware_features(test)\n\n# =============================================================================\n# 2. 基础统计特征 (Basic Statistical Features)\n# =============================================================================\n\ndef extract_basic_features(df):\n    \"\"\"\n    提取基础统计特征（类似Step 1但更全面）\n    \"\"\"\n    print(\"提取基础统计特征...\")\n    features = pd.DataFrame(index=df.index)\n    \n    # 长度特征\n    features['prompt_length'] = df['prompt'].str.len()\n    features['response_a_length'] = df['response_a'].str.len()\n    features['response_b_length'] = df['response_b'].str.len()\n    \n    # 单词数量\n    features['prompt_word_count'] = df['prompt'].str.split().str.len()\n    features['response_a_word_count'] = df['response_a'].str.split().str.len()\n    features['response_b_word_count'] = df['response_b'].str.split().str.len()\n    \n    # 标点符号数量\n    import string\n    features['prompt_punc_count'] = df['prompt'].str.count(f'[{re.escape(string.punctuation)}]')\n    features['response_a_punc_count'] = df['response_a'].str.count(f'[{re.escape(string.punctuation)}]')\n    features['response_b_punc_count'] = df['response_b'].str.count(f'[{re.escape(string.punctuation)}]')\n    \n    # 差异特征\n    features['response_length_diff'] = features['response_a_length'] - features['response_b_length']\n    features['response_word_diff'] = features['response_a_word_count'] - features['response_b_word_count']\n    features['response_punc_diff'] = features['response_a_punc_count'] - features['response_b_punc_count']\n    \n    print(f\"基础特征提取完成，特征数量: {features.shape[1]}\")\n    return features\n\n# 提取基础特征\nbasic_features_train = extract_basic_features(train)\nbasic_features_test = extract_basic_features(test)\n\n# =============================================================================\n# 3. 嵌入特征 (Embedding Features)\n# =============================================================================\n\nprint(\"加载嵌入模型...\")\n# 使用小组已有的预训练模型\ntry:\n    model = SentenceTransformer('/kaggle/input/minilm-l12-v2-local/other/default/1/minilm_l12_v2_local')\n    print(\"使用本地预训练模型\")\nexcept:\n    # 如果本地模型不可用，使用在线模型\n    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n    print(\"使用在线预训练模型\")\n\ndef create_embedding_features(df, model):\n    \"\"\"\n    创建嵌入特征\n    \"\"\"\n    print(\"生成嵌入特征...\")\n    \n    # 文本拼接\n    def concat_text(df):\n        return (\n            df['prompt'].astype(str) + ' [SEP] ' + df['response_a'].astype(str),\n            df['prompt'].astype(str) + ' [SEP] ' + df['response_b'].astype(str)\n        )\n    \n    text_a, text_b = concat_text(df)\n    \n    # 获取嵌入特征\n    emb_a = model.encode(text_a.tolist(), batch_size=32, show_progress_bar=True)\n    emb_b = model.encode(text_b.tolist(), batch_size=32, show_progress_bar=True)\n    \n    # 嵌入特征工程\n    embedding_features = np.hstack([\n        emb_a, \n        emb_b, \n        emb_a - emb_b,  # 差异特征\n        np.abs(emb_a - emb_b),  # 绝对差异\n        emb_a * emb_b,  # 交互特征\n    ])\n    \n    print(f\"嵌入特征生成完成，特征维度: {embedding_features.shape}\")\n    return embedding_features\n\n# 创建嵌入特征\nprint(\"为训练集创建嵌入特征...\")\nembedding_features_train = create_embedding_features(train, model)\nprint(\"为测试集创建嵌入特征...\")\nembedding_features_test = create_embedding_features(test, model)\n\n# =============================================================================\n# 4. 合并所有特征\n# =============================================================================\n\nprint(\"合并所有特征...\")\nX_ensemble = np.hstack([\n    embedding_features_train,\n    bias_features_train.values,\n    basic_features_train.values\n])\n\nX_test_ensemble = np.hstack([\n    embedding_features_test,\n    bias_features_test.values,\n    basic_features_test.values\n])\n\n# 目标变量\ny = train[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)\n\nprint(f\"最终特征矩阵大小: {X_ensemble.shape}\")\nprint(f\"测试集特征矩阵大小: {X_test_ensemble.shape}\")\n\n# =============================================================================\n# 5. 数据准备\n# =============================================================================\n\nprint(\"准备训练和验证数据...\")\nX_train, X_val, y_train, y_val = train_test_split(\n    X_ensemble, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 特征标准化\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test_ensemble)\n\nprint(\"数据准备完成\")\n\n# =============================================================================\n# 6. 训练多个基础模型\n# =============================================================================\n\nprint(\"训练多个基础模型...\")\n\n# 基础模型集合\nmodels = {\n    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n    'XGBoost': xgb.XGBClassifier(\n        n_estimators=100,\n        learning_rate=0.1,\n        random_state=42,\n        eval_metric='mlogloss',\n        n_jobs=-1\n    ),\n    'LightGBM': lgb.LGBMClassifier(\n        n_estimators=100,\n        learning_rate=0.1,\n        random_state=42,\n        n_jobs=-1\n    )\n}\n\n# 单独训练每个模型并评估\nbase_model_predictions = {}\nbase_model_scores = {}\n\nfor name, model in models.items():\n    print(f\"训练 {name}...\")\n    model.fit(X_train_scaled, y_train)\n    \n    # 验证集预测\n    val_pred_proba = model.predict_proba(X_val_scaled)\n    val_score = log_loss(y_val, val_pred_proba)\n    \n    base_model_predictions[name] = val_pred_proba\n    base_model_scores[name] = val_score\n    print(f\"  {name} 验证集 Log Loss: {val_score:.4f}\")\n\n# =============================================================================\n# 7. 集成方法\n# =============================================================================\n\nprint(\"\\n训练集成模型...\")\n\n# 7.1 投票集成 (Voting Ensemble)\nprint(\"训练投票集成模型...\")\nvoting_clf = VotingClassifier(\n    estimators=[(name, model) for name, model in models.items()],\n    voting='soft'\n)\n\nvoting_clf.fit(X_train_scaled, y_train)\nvoting_pred_proba = voting_clf.predict_proba(X_val_scaled)\nvoting_score = log_loss(y_val, voting_pred_proba)\nprint(f\"投票集成验证集 Log Loss: {voting_score:.4f}\")\n\n# 7.2 概率校准 (Probability Calibration)\nprint(\"进行概率校准...\")\n# 选择性能最好的基础模型进行校准\nbest_base_model_name = min(base_model_scores, key=base_model_scores.get)\nprint(f\"选择 {best_base_model_name} 进行概率校准\")\n\ncalibrated_clf = CalibratedClassifierCV(\n    models[best_base_model_name], \n    method='isotonic', \n    cv=3\n)\ncalibrated_clf.fit(X_train_scaled, y_train)\ncalibrated_pred_proba = calibrated_clf.predict_proba(X_val_scaled)\ncalibrated_score = log_loss(y_val, calibrated_pred_proba)\nprint(f\"校准后验证集 Log Loss: {calibrated_score:.4f}\")\n\n# 7.3 加权平均集成 (Weighted Average Ensemble)\nprint(\"进行加权平均集成...\")\n# 基于验证集性能计算权重（性能越好权重越高）\nweights = {}\ntotal_performance = sum(1/score for score in base_model_scores.values())\nfor name, score in base_model_scores.items():\n    weights[name] = (1/score) / total_performance\n\nprint(\"模型权重分配:\")\nfor name, weight in weights.items():\n    print(f\"  {name}: {weight:.3f}\")\n\ndef weighted_average_predict(models, weights, X):\n    \"\"\"加权平均预测\"\"\"\n    predictions = []\n    for name, model in models.items():\n        pred = model.predict_proba(X)\n        weighted_pred = pred * weights[name]\n        predictions.append(weighted_pred)\n    \n    # 平均预测\n    final_pred = np.mean(predictions, axis=0)\n    return final_pred\n\nweighted_pred_proba = weighted_average_predict(models, weights, X_val_scaled)\nweighted_score = log_loss(y_val, weighted_pred_proba)\nprint(f\"加权平均集成验证集 Log Loss: {weighted_score:.4f}\")\n\n# =============================================================================\n# 8. 模型性能比较和选择\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"模型性能比较\")\nprint(\"=\"*50)\n\n# 收集所有模型性能\nall_scores = {\n    **base_model_scores,\n    'Voting Ensemble': voting_score,\n    'Calibrated Model': calibrated_score,\n    'Weighted Average': weighted_score\n}\n\n# 按性能排序\nsorted_scores = sorted(all_scores.items(), key=lambda x: x[1])\n\nprint(\"\\n模型性能排名:\")\nprint(\"-\" * 40)\nfor name, score in sorted_scores:\n    print(f\"{name:20} | Log Loss: {score:.4f}\")\n\n# 选择最佳模型\nbest_model_name, best_score = sorted_scores[0]\nprint(f\"\\n最佳模型: {best_model_name}\")\nprint(f\"最佳分数: {best_score:.4f}\")\n\n# =============================================================================\n# 9. 使用最佳模型进行最终预测\n# =============================================================================\n\nprint(f\"\\n使用 {best_model_name} 进行最终预测...\")\n\nif best_model_name == 'Voting Ensemble':\n    final_model = voting_clf\n    # 在所有数据上重新训练\n    X_all_scaled = scaler.fit_transform(X_ensemble)\n    final_model.fit(X_all_scaled, y)\n    final_predictions = final_model.predict_proba(X_test_scaled)\n    \nelif best_model_name == 'Calibrated Model':\n    final_model = calibrated_clf\n    # 注意：CalibratedClassifierCV已经使用了交叉验证，不需要重新训练\n    final_predictions = final_model.predict_proba(X_test_scaled)\n    \nelif best_model_name == 'Weighted Average':\n    # 对于加权平均，在所有数据上重新训练基础模型\n    print(\"在所有数据上重新训练基础模型用于加权平均...\")\n    for name, model in models.items():\n        X_all_scaled = scaler.fit_transform(X_ensemble)\n        model.fit(X_all_scaled, y)\n    final_predictions = weighted_average_predict(models, weights, X_test_scaled)\n    \nelse:\n    # 单个基础模型\n    final_model = models[best_model_name]\n    X_all_scaled = scaler.fit_transform(X_ensemble)\n    final_model.fit(X_all_scaled, y)\n    final_predictions = final_model.predict_proba(X_test_scaled)\n\n# =============================================================================\n# 10. 生成提交文件\n# =============================================================================\n\nprint(\"生成提交文件...\")\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': final_predictions[:, 0],\n    'winner_model_b': final_predictions[:, 1],\n    'winner_tie': final_predictions[:, 2],\n})\n\nsubmission.to_csv('submission_advanced_ensemble.csv', index=False)\nprint(\"高级集成模型提交文件已生成: submission_advanced_ensemble.csv\")\n\nprint(\"\\n提交文件预览:\")\nprint(submission.head())\n\n# =============================================================================\n# 11. 误差分析 (修复后的版本)\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"误差分析\")\nprint(\"=\"*50)\n\n# 使用最佳模型在验证集上的预测进行分析\nif best_model_name == 'Weighted Average':\n    val_predictions_final = weighted_average_predict(models, weights, X_val_scaled)\nelse:\n    val_predictions_final = final_model.predict_proba(X_val_scaled)\n\nval_pred_labels = np.argmax(val_predictions_final, axis=1)\n\n# 计算每个类别的log loss (修复：添加labels参数)\nclass_names = ['model_a', 'model_b', 'tie']\nprint(\"\\n各类别Log Loss分析:\")\nfor class_idx in range(3):\n    class_mask = (y_val == class_idx)\n    if np.sum(class_mask) > 0:\n        # 修复：添加labels参数，明确指定所有可能的类别\n        class_loss = log_loss(y_val[class_mask], val_predictions_final[class_mask], labels=[0, 1, 2])\n        print(f\"  类别 {class_names[class_idx]}: {class_loss:.4f}\")\n    else:\n        print(f\"  类别 {class_names[class_idx]}: 无样本\")\n\n# 混淆矩阵\ncm = confusion_matrix(y_val, val_pred_labels)\nprint(f\"\\n混淆矩阵:\")\nprint(cm)\n\n# 准确率\naccuracy = np.mean(y_val == val_pred_labels)\nprint(f\"\\n验证集准确率: {accuracy:.4f}\")\n\n# =============================================================================\n# 12. 与基线模型比较\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Step 3 完成总结\")\nprint(\"=\"*50)\n\nprint(\"Step 3 高级集成模型完成!\")\nprint(f\"最佳模型: {best_model_name}\")\nprint(f\"验证集Log Loss: {best_score:.4f}\")\nprint(f\"验证集准确率: {accuracy:.4f}\")\nprint(\"提交文件: submission_advanced_ensemble.csv\")\n\nprint(\"\\n请将此文件提交到Kaggle查看最终分数!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}